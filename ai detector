import torch

from transformers import AutoModelForSequenceClassification, AutoTokenizer



# Load optimized AI detection model

model_name = "microsoft/deberta-v3-base"  # A better alternative to BERT for classification

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSequenceClassification.from_pretrained(model_name).to("mps" if torch.backends.mps.is_available() else "cpu")



def detect_ai_text(text):

    device = "mps" if torch.backends.mps.is_available() else "cpu"

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)

    with torch.no_grad():

        logits = model(**inputs).logits

    probabilities = torch.nn.functional.softmax(logits, dim=1)

    ai_probability = probabilities[0][1].item()



    return "AI-Generated" if ai_probability > 0.5 else "Human-Written"



if __name__ == "__main__":

    text = input("Enter text to check: ")

    print("Result:", detect_ai_text(text))
